from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import re
import csv

# ===== í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • =====
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome(options=options)

# ===== URL ì„¤ì • =====
base_url = "https://padm.kangwon.ac.kr"
list_url = f"{base_url}/padm/life/notice-department.do"

# ===== HTML íƒœê·¸ ì œê±° ë° í‘œ ì²˜ë¦¬ =====
def clean_html_keep_table(raw_html):
    soup = BeautifulSoup(raw_html, 'html.parser')

    output_text = ''

    # ğŸ”¥ 1. í…Œì´ë¸” ë¨¼ì € ì¶”ì¶œ
    tables = soup.find_all('table')
    for table in tables:
        table_text = extract_table_text(table)
        if table_text.strip():
            output_text += table_text + '\n'
        table.decompose()  # âœ… í…Œì´ë¸” ì œê±° (ì¤‘ë³µ ë°©ì§€)

    # ğŸ”¥ 2. ë‚¨ì€ ë³¸ë¬¸ (p, div ë“±) ì¶”ì¶œ
    for elem in soup.find_all(['p', 'div']):
        text = elem.get_text(strip=True)
        if text:
            output_text += text + '\n'

    return output_text.strip()



def extract_table_text(table):
    rows = table.find_all('tr')
    table_text = ''
    for row in rows:
        cols = row.find_all(['td', 'th'])
        valid_cols = [col.get_text(strip=True) for col in cols if col.get_text(strip=True)]
        if valid_cols:
            row_text = ' | '.join(valid_cols)
            table_text += row_text + '\n'
    return table_text


# ===== ê³µì§€ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ =====
def crawl_notice_list(offset=0):
    driver.get(f"{list_url}?article.offset={offset}")
    time.sleep(2)

    notices = []
    rows = driver.find_elements(By.CSS_SELECTOR, 'td.b-td-left.b-td-title')

    for row in rows:
        try:
            link_tag = row.find_element(By.CSS_SELECTOR, 'div.b-title-box a')
            title = link_tag.text.strip()
            href = link_tag.get_attribute('href')
            detail_url = base_url + "/padm/life/notice-department.do" + href[href.find('?'):]
            notices.append({'title': title, 'url': detail_url})
        except Exception as e:
            print("[!] ë¦¬ìŠ¤íŠ¸ í•­ëª© íŒŒì‹± ì‹¤íŒ¨:", e)
            continue

    return notices

# ===== ê³µì§€ ë³¸ë¬¸ í¬ë¡¤ë§ (ë³¸ë¬¸ + í‘œ ì²˜ë¦¬) =====
def crawl_notice_detail(url):
    driver.get(url)

    selector_candidates = [
        'div.b-content-box div.fr-view',
        'div.b-content-box'
    ]

    for selector in selector_candidates:
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            element = driver.find_element(By.CSS_SELECTOR, selector)
            content_html = element.get_attribute('innerHTML')
            content_text = clean_html_keep_table(content_html)
            if content_text.strip():
                return content_text
        except:
            continue

    return "(ë³¸ë¬¸ ì—†ìŒ)"

# ===== ë©”ì¸ ì‹¤í–‰ =====
if __name__ == "__main__":
    all_notices = []

    # âœ… ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§ (ì˜ˆì‹œ: 1~2í˜ì´ì§€ë§Œ)
    for offset in range(0, 20, 10):  # 10ê°œ ë‹¨ìœ„ë¡œ: 0, 10, 20, ...
        notices = crawl_notice_list(offset=offset)

        for notice in notices:
            title = notice['title']
            url = notice['url']
            content = crawl_notice_detail(url)

            all_notices.append({'ì œëª©': title, 'ë³¸ë¬¸': content})

            print("==== ì œëª© ====")
            print('â— '+title)
            print("==== ë³¸ë¬¸ ====")
            print(content)
            print("\n\n")

    driver.quit()
